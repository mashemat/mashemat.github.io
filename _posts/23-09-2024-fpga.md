---
title: "Architecting in-network memory access"
categories:
  - Blog
tags:
  - FPGA
  - Post Formats
---

<p align="center" style="font-size: 14px; width: 80%; margin: auto;">
In modern data centers, network layer devices with computation abilities are predominantly implemented as SoCs and ASICs for custom hardware solutions. These programmable yet fixed-hardware solutions might offer robust performance for their intended purpose they can be less adaptable to rapidly changing data center requirements compared to more flexible solutions like FPGAs. For instance, ASICs designed for handling IPV4 protocol might need to be replaced with a re-designed ASIC/SoC card to be compatible with IPV6 traffic. This is where configurable hardware comes into the picture, where FPGAs are deployed and configured/re-configured on the go to handle the changing demands of the network. One prominent example of programmable silicon hardware i.e FPGAs being utilized in cloud computing to augment CPU performance is Microsoftâ€™s Project Catapult whose nodes are powered
by Altera Stratix V D5 FPGAs. 
</p>


<p align="center">
  <img src="/assets/images/fpga.png" alt="InfiniBand Network"  width="600" height="300" />
</p>

<p align="center" style="font-size: 14px; width: 80%; margin: auto;">
We intended to explore FPGA-based SmartNICs, which are configurable hardware devices in contrast to SoC and ASIC solutions, used in in-network computing for offloading the tasks from the host to the network
itself. One of the most crucial requirements of offloading tasks between devices on the network is efficient memory access, which often encounters bottlenecks due to limitations in communication protocols and data transfer
methods.
</p>

